--- arch/riscv/core/isr.S
+++ arch/riscv/core/isr.S
@@ -131,23 +131,77 @@ SECTION_FUNC(exception.entry, __irq_wrapper)
 	RV_OP_STOREREG t0, __z_arch_esf_t_t0_OFFSET(sp)
 	RV_OP_STOREREG t1, __z_arch_esf_t_t1_OFFSET(sp)
 	RV_OP_STOREREG t2, __z_arch_esf_t_t2_OFFSET(sp)
+#ifndef CONFIG_EMBEDDED_ISA
 	RV_OP_STOREREG t3, __z_arch_esf_t_t3_OFFSET(sp)
 	RV_OP_STOREREG t4, __z_arch_esf_t_t4_OFFSET(sp)
 	RV_OP_STOREREG t5, __z_arch_esf_t_t5_OFFSET(sp)
 	RV_OP_STOREREG t6, __z_arch_esf_t_t6_OFFSET(sp)
+#endif
 	RV_OP_STOREREG a0, __z_arch_esf_t_a0_OFFSET(sp)
 	RV_OP_STOREREG a1, __z_arch_esf_t_a1_OFFSET(sp)
 	RV_OP_STOREREG a2, __z_arch_esf_t_a2_OFFSET(sp)
 	RV_OP_STOREREG a3, __z_arch_esf_t_a3_OFFSET(sp)
 	RV_OP_STOREREG a4, __z_arch_esf_t_a4_OFFSET(sp)
 	RV_OP_STOREREG a5, __z_arch_esf_t_a5_OFFSET(sp)
+#ifndef CONFIG_EMBEDDED_ISA
 	RV_OP_STOREREG a6, __z_arch_esf_t_a6_OFFSET(sp)
 	RV_OP_STOREREG a7, __z_arch_esf_t_a7_OFFSET(sp)
+#endif
 
 #if defined(CONFIG_FPU) && defined(CONFIG_FPU_SHARING)
 	/* Assess whether floating-point registers need to be saved. */
+#ifdef CONFIG_USE_SWITCH
+	/*
+	 * In CONFIG_USE_SWITCH, reschedule system call handler can only
+	 * get current thread (old thread) via register a1 instead _current.
+	 */
+	jal ra, __soc_is_irq
+
+	/* If a0 != 0, it is an interrupt but not a system call */
+	addi t1, x0, 0
+	bnez a0, not_resch_syscall
+
+	/* If mcause != SOC_MCAUSE_ECALL_EXP, it is not a system call. */
+	csrr t0, mcause
+	li t2, SOC_MCAUSE_EXP_MASK
+	and t0, t0, t2
+	li t1, SOC_MCAUSE_ECALL_EXP
+	bne t0, t1, not_resch_syscall
+
+#ifdef CONFIG_IRQ_OFFLOAD
+	/*
+	 * Determine if the system call is the result of an IRQ offloading.
+	 * Done by checking if _offload_routine is not pointing to NULL.
+	 * If NULL, it is reschedule system call.
+	 */
+	la t0, _offload_routine
+	RV_OP_LOADREG t1, 0x00(t0)
+	bnez t1, not_resch_syscall
+#endif
+
+	/*
+	 * If the trap is reschedule syscall, old thread is NOT at
+	 * _current but at register a1.
+	 */
+	RV_OP_LOADREG t1, __z_arch_esf_t_a1_OFFSET(sp)
+
+	/*
+	 * Get old thread's TCB pointer from it's switch_handle member pointer.
+	 * (use container_of() technique)
+	 */
+	addi t0, t1, -___thread_t_switch_handle_OFFSET
+	jal store_fp_caller_saved
+
+not_resch_syscall:
+#endif /* CONFIG_USE_SWITCH */
+#ifdef CONFIG_SMP
+	csrr t0, mscratch
+#else
 	la t0, _kernel
-	RV_OP_LOADREG t0, _kernel_offset_to_current(t0)
+#endif
+	RV_OP_LOADREG t0, ___cpu_t_current_OFFSET(t0)
+
+store_fp_caller_saved:
 	RV_OP_LOADREG t0, _thread_offset_to_user_options(t0)
 	andi t0, t0, K_FP_REGS
 	RV_OP_STOREREG t0, __z_arch_esf_t_fp_state_OFFSET(sp)
@@ -240,10 +294,31 @@ is_syscall:
 	bnez t1, is_interrupt
 #endif
 
+#ifdef CONFIG_USE_SWITCH
+	/*
+	 * arguments are from arch_switch(switch_to, switched_from)
+	 *
+	 * Save sp register to switched_from
+	 * Set switch_to to a0 register
+	 */
+	RV_OP_LOADREG t1, __z_arch_esf_t_a1_OFFSET(sp)
+	RV_OP_STOREREG sp, 0x00(t1)
+	RV_OP_LOADREG a0, __z_arch_esf_t_a0_OFFSET(sp)
+
+	/*
+	 * Get old thread's TCB pointer from it's switch_handle member pointer.
+	 * (use container_of() technique)
+	 */
+	addi a1, t1, -___thread_t_switch_handle_OFFSET
+
+	/* Go to reschedule_switch to handle context-switch */
+	j reschedule_switch
+#else
 	/*
 	 * Go to reschedule to handle context-switch
 	 */
 	j reschedule
+#endif /* CONFIG_USE_SWITCH */
 
 is_interrupt:
 	/*
@@ -255,8 +330,12 @@ is_interrupt:
 	addi t0, sp, 0
 
 	/* Switch to interrupt stack */
+#ifdef CONFIG_SMP
+	csrr t2, mscratch
+#else
 	la t2, _kernel
-	RV_OP_LOADREG sp, _kernel_offset_to_irq_stack(t2)
+#endif
+	RV_OP_LOADREG sp, ___cpu_t_irq_stack_OFFSET(t2)
 
 	/*
 	 * Save thread stack pointer on interrupt stack
@@ -266,10 +345,10 @@ is_interrupt:
 	RV_OP_STOREREG t0, 0x00(sp)
 
 on_irq_stack:
-	/* Increment _kernel.cpus[0].nested variable */
-	lw t3, _kernel_offset_to_nested(t2)
-	addi t3, t3, 1
-	sw t3, _kernel_offset_to_nested(t2)
+	/* Increment _kernel.nested variable */
+	lw a3, ___cpu_t_nested_OFFSET(t2)
+	addi a3, a3, 1
+	sw a3, ___cpu_t_nested_OFFSET(t2)
 
 #ifdef CONFIG_IRQ_OFFLOAD
 	/*
@@ -322,12 +401,16 @@ call_irq:
 
 on_thread_stack:
 	/* Get reference to _kernel */
+#ifdef CONFIG_SMP
+	csrr t1, mscratch
+#else
 	la t1, _kernel
+#endif
 
 	/* Decrement _kernel.cpus[0].nested variable */
-	lw t2, _kernel_offset_to_nested(t1)
+	lw t2, ___cpu_t_nested_OFFSET(t1)
 	addi t2, t2, -1
-	sw t2, _kernel_offset_to_nested(t1)
+	sw t2, ___cpu_t_nested_OFFSET(t1)
 
 	/* Restore thread stack pointer */
 	RV_OP_LOADREG t0, 0x00(sp)
@@ -335,36 +418,83 @@ on_thread_stack:
 
 #ifdef CONFIG_STACK_SENTINEL
 	call z_check_stack_sentinel
+#ifdef CONFIG_SMP
+	csrr t1, mscratch
+#else
 	la t1, _kernel
 #endif
+#endif
 
 #ifdef CONFIG_PREEMPT_ENABLED
 	/*
 	 * Check if we need to perform a reschedule
 	 */
 
+#ifdef CONFIG_USE_SWITCH
+	/* Get old thread TCB pointer (from _kernel.current) and save to stack */
+	RV_OP_LOADREG t0, ___cpu_t_current_OFFSET(t1)
+	addi sp, sp, -16
+	RV_OP_STOREREG t0, 0x00(sp)
+
+	/* Set thread stack address (exclude saving old thread) to interrupted */
+	addi a0, sp, 16
+	call z_get_next_switch_handle
+
+	/* Restore old thread TCB pointer and save to a1 */
+	RV_OP_LOADREG a1, 0x00(sp)
+	addi sp, sp, 16
+
+	/*
+	 * Check if next thread to schedule is current thread.
+	 * If yes do not perform a reschedule
+	 */
+	beq a0, sp, no_reschedule
+#else /* CONFIG_USE_SWITCH */
 	/* Get pointer to _kernel.current */
-	RV_OP_LOADREG t2, _kernel_offset_to_current(t1)
+	RV_OP_LOADREG t2, ___cpu_t_current_OFFSET(t1)
 
 	/*
 	 * Check if next thread to schedule is current thread.
 	 * If yes do not perform a reschedule
 	 */
-	RV_OP_LOADREG t3, _kernel_offset_to_ready_q_cache(t1)
-	beq t3, t2, no_reschedule
+	RV_OP_LOADREG a3, _kernel_offset_to_ready_q_cache(t1)
+	beq a3, t2, no_reschedule
+#endif /* CONFIG_USE_SWITCH */
+
 #else
 	j no_reschedule
 #endif /* CONFIG_PREEMPT_ENABLED */
 
+#ifdef CONFIG_USE_SWITCH
+/*
+ * reschedule_switch(void* switch_to, k_thread* from_thread)
+ *
+ * switch_to is stored in a0 register.
+ * from_thread is stored in a1 register.
+ */
+reschedule_switch:
+#else
 reschedule:
+#endif /* CONFIG_USE_SWITCH */
+
 #if CONFIG_TRACING
 	call sys_trace_thread_switched_out
 #endif
+
+#ifdef CONFIG_USE_SWITCH
+	/* Get old thread from a1 */
+	mv t1, a1
+#else
 	/* Get reference to _kernel */
+#ifdef CONFIG_SMP
+	csrr t0, mscratch
+#else
 	la t0, _kernel
+#endif
 
 	/* Get pointer to _kernel.current */
-	RV_OP_LOADREG t1, _kernel_offset_to_current(t0)
+	RV_OP_LOADREG t1, ___cpu_t_current_OFFSET(t0)
+#endif
 
 	/*
 	 * Save callee-saved registers of current thread
@@ -395,14 +527,33 @@ reschedule:
 skip_store_fp_callee_saved:
 #endif
 
+#ifdef CONFIG_SMP_HOTFIX_SPIN_ON_RISCV_CALLEE
+	/* All callee registers are saved. */
+	sw x0, _thread_offset_to_callee_state(t1)
+#endif
+
+#ifdef CONFIG_USE_SWITCH
+	/* Get reference to _kernel */
+#ifdef CONFIG_SMP
+	csrr t0, mscratch
+#else
+	la t0, _kernel
+#endif
+
+	/* Get next thread from _kernel.current and save to t1 */
+	RV_OP_LOADREG t1, ___cpu_t_current_OFFSET(t0)
+
+	/* Switch to new thread stack */
+	mv sp, a0
+#else
 	/*
 	 * Save stack pointer of current thread and set the default return value
 	 * of z_swap to _k_neg_eagain for the thread.
 	 */
 	RV_OP_STOREREG sp, _thread_offset_to_sp(t1)
 	la t2, _k_neg_eagain
-	lw t3, 0x00(t2)
-	sw t3, _thread_offset_to_swap_return_value(t1)
+	lw a3, 0x00(t2)
+	sw a3, _thread_offset_to_swap_return_value(t1)
 
 	/* Get next thread to schedule. */
 	RV_OP_LOADREG t1, _kernel_offset_to_ready_q_cache(t0)
@@ -410,14 +561,25 @@ skip_store_fp_callee_saved:
 	/*
 	 * Set _kernel.current to new thread loaded in t1
 	 */
-	RV_OP_STOREREG t1, _kernel_offset_to_current(t0)
+	RV_OP_STOREREG t1, ___cpu_t_current_OFFSET(t0)
 
 	/* Switch to new thread stack */
 	RV_OP_LOADREG sp, _thread_offset_to_sp(t1)
+#endif
+
+#ifdef CONFIG_SMP_HOTFIX_SPIN_ON_RISCV_CALLEE
+	li t0, 1
+	addi t2, t1, _thread_offset_to_callee_state
+callee_spin:
+	/* Acquire callee state lock */
+	amoswap.w.aq t0, t0, (t2)
+	bnez t0, callee_spin
+#endif /* CONFIG_SMP_HOTFIX_SPIN_ON_RISCV_CALLEE */
 
 	/* Restore callee-saved registers of new thread */
 	RV_OP_LOADREG s0, _thread_offset_to_s0(t1)
 	RV_OP_LOADREG s1, _thread_offset_to_s1(t1)
+#ifndef CONFIG_EMBEDDED_ISA
 	RV_OP_LOADREG s2, _thread_offset_to_s2(t1)
 	RV_OP_LOADREG s3, _thread_offset_to_s3(t1)
 	RV_OP_LOADREG s4, _thread_offset_to_s4(t1)
@@ -489,18 +652,22 @@ skip_load_fp_caller_saved:
 	RV_OP_LOADREG t0, __z_arch_esf_t_t0_OFFSET(sp)
 	RV_OP_LOADREG t1, __z_arch_esf_t_t1_OFFSET(sp)
 	RV_OP_LOADREG t2, __z_arch_esf_t_t2_OFFSET(sp)
+#ifndef CONFIG_EMBEDDED_ISA
 	RV_OP_LOADREG t3, __z_arch_esf_t_t3_OFFSET(sp)
 	RV_OP_LOADREG t4, __z_arch_esf_t_t4_OFFSET(sp)
 	RV_OP_LOADREG t5, __z_arch_esf_t_t5_OFFSET(sp)
 	RV_OP_LOADREG t6, __z_arch_esf_t_t6_OFFSET(sp)
+#endif
 	RV_OP_LOADREG a0, __z_arch_esf_t_a0_OFFSET(sp)
 	RV_OP_LOADREG a1, __z_arch_esf_t_a1_OFFSET(sp)
 	RV_OP_LOADREG a2, __z_arch_esf_t_a2_OFFSET(sp)
 	RV_OP_LOADREG a3, __z_arch_esf_t_a3_OFFSET(sp)
 	RV_OP_LOADREG a4, __z_arch_esf_t_a4_OFFSET(sp)
 	RV_OP_LOADREG a5, __z_arch_esf_t_a5_OFFSET(sp)
+#ifndef CONFIG_EMBEDDED_ISA
 	RV_OP_LOADREG a6, __z_arch_esf_t_a6_OFFSET(sp)
 	RV_OP_LOADREG a7, __z_arch_esf_t_a7_OFFSET(sp)
+#endif
 
 	/* Release stack space */
 	addi sp, sp, __z_arch_esf_t_SIZEOF
